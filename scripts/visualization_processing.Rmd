---
title: "ASReview FTM Hackathon - Visualization of email domains with VOSViewer"
output: html_notebook
author: Bianca Kramer
date: 2021-11-28
---

## 0. Load packages
```{r message=FALSE, warning=FALSE}

library(jsonlite)
library(readxl)
library(tidytext)
library(tidyverse)

```

## 1. Identifying email domains 

## 1a. Read source data 
(n=2509 records)
```{r message=FALSE, warning=FALSE} 

data_preprocessed <- read_excel("../data/preprocessed_data.xlsx", 
                    col_types = c("numeric", "text", "text", 
                                  "text", "date", "text", "text"))

data_preprocessed <- data_preprocessed %>%
  rename(seqID = 1)

#2509 records

#NB column date has inconsistent formatting, but almost all emails have a date in column betterDate

```

## 1b. Retrieve sender and recipient domains from emails
(2109 mail records, 1429 with likely email addresses detected)
```{r message=FALSE, warning=FALSE}

#fiter emails (n=2109 records)
data <- data_preprocessed %>%
  filter(type == "Mail")

#convert body of emails into separate lines (n=291089 lines)
data <- data %>%
  tidytext::unnest_tokens(lines, abstract, token = "lines", drop = FALSE)

#keep lines with (likely) email addresses (n=9628 lines)
#TODO optimize strings used in filter
selection_terms <- c("@", 
                     "to:", "to :", 
                     "from:", "from :",
                     "aan:", "aan :",
                     "van:", "van :")
selection <- str_flatten(selection_terms, "|")

data <- data %>%
  filter(stringr::str_detect(lines, selection))

#check how many original records remain
count_records <- data %>%
  count(id) %>%
  nrow()
#n=1884 (of 2109) records with likely email addresses

#convert remaining lines into separate words (n = 43925 lines)
#NB Tokenizing words removes punctuation (such as @) but keeps periods (.)
#This suits this particular purpose well but good to be aware of
data <- data %>%
  tidytext::unnest_tokens(strings, lines, token = "words", drop = FALSE)

#filter email adress domains xxx.xx  (n= 8732 lines)
data <- data %>%
  filter(stringr::str_detect(strings, "\\.")) 

#check how many original records remain
count_records <- data %>%
  count(id) %>%
  nrow()
#n=1429 (of 1884) records with likely email addresses

rm(selection, selection_terms)

#TODO keep to/from indications to provide directionality
#but many records miss to/from indication, so will loose those

#Note: records with email threads can have multiple to/from email domain strings; records with multiple recipients will also result in multiple email domain strings per record

```

# 1c. Further cleaning of domain strings
(1366 records with 917 unique email domains)

```{r message=FALSE, warning=FALSE}

#read current list of top level domains
tld <- read_tsv("https://data.iana.org/TLD/tlds-alpha-by-domain.txt") %>%
  rename(domain = 1) %>%
  mutate(domain = str_to_lower(domain)) %>%
  pull(domain)

data_clean <- data %>%
  #remove strings with > 1 periods (urls, email prefixes etc)
  filter(str_count(strings, pattern = "\\.") == 1) %>%
  #split domains into prefix and suffix
  separate(strings, c("prefix", "suffix"), sep = "\\.", remove = FALSE) %>%
  #filter on domain suffixes that are valid top level domains
  filter(suffix %in% tld)

#check how many original records remain
count_records <- data_clean %>%
  count(id) %>%
  nrow()
#n=1386 (of 1429) records with likely email addresses

#check how many unique domain strings 
count_strings <- data_clean %>%
  count(strings) %>%
  nrow()
#n=917 (of 7647) unique emai domain strings

rm(tld)

#TODO further clean domain prefixes

```


# 2. Prepare data for VOSviewer  
Create JSON object according to these specs:   https://app.vosviewer.com/docs/file-types/json-file-type/

## 2a. Keep each domain only once per record
NB Reconsider this when creating a directional graph

```{r message=FALSE, warning=FALSE}

data_single <- data_clean %>%
  select(strings, id) %>%
  distinct()

```

## 2b. First element in JSON object: node list (called 'items')
917 list elements

```{r message=FALSE, warning=FALSE}

#create df of unique domains with weights as list column
df_nodes <- data_single %>%
  count(strings) %>%
  distinct() %>%
  arrange(strings) %>%
  mutate(id = row_number()) %>%
  rename(label = strings,
         Documents = n) %>%
  select(id, label, Documents)

#transpose to list
items <- df_nodes %>%
  purrr::transpose()

#define helper function to modify list elements
modifyList <- function(x){
  x["weights"] <- list(x["Documents"])
  x["Documents"] <- NULL
  return(x)
}

#apply helper function to each list element
items <- map(items, modifyList)

```


## 2c. Second element in JSON object: edge list (called 'links')
12639 list elements

```{r message=FALSE, warning=FALSE}

#only keep records with >1 email domain (n=808)
count_records <- data_single %>%
  count(id) %>%
  filter(n > 1) %>%
  pull(id)

combinations <- data_single %>%
  filter(id %in% count_records)

nodes <- combinations %>%
  pull(strings) %>%
  unique()

rm(count_records)

#create list for edges and combinations
list <- list(edges = NULL,
             combinations = combinations)

#iterate over vector with nodes to create edge list for each node
#remove node from dataframe before next iteration
for (i in nodes) {

  nodes1 <- list$combinations %>%
    filter(strings == i) %>%
    rename(source = strings)
  
  nodes2 <- list$combinations %>%
    filter(!strings == i) %>%
    rename(target = strings)
  
  edges <- nodes1 %>%
    left_join(nodes2, by = "id") %>%
    count(source, target)
  
  combinations <- list$combinations %>%
    filter(!strings == i)
  
  list$combinations <- combinations
  list$edges <- bind_rows(list$edges, edges)

}

#modify dataframe with edge list
df_edges <- list$edges %>%
  filter(!is.na(target)) %>%
  rename(strength = n)

#replace source/target strings with source/target ids matching those in node list
ids <- df_nodes %>%
  select(label, id)
  
df_edges <- df_edges %>%
  left_join(ids, by = c("source" = "label")) %>%
  rename(source_id = id) %>%
  left_join(ids, by = c("target" = "label")) %>%
  rename(target_id = id) %>%
  select(source_id, target_id, strength)
  
#create list
links <- df_edges %>%  
  purrr::transpose()

source_id <- links %>%
  select(source_id) %>%
  distinct() %>%

target_id <- links %>%
  select(target_id) %>%
  distinct()

source_id_match <- source_id %>%
  left_join(df_nodes, by = c("source_id" = "label"))

target_id_match <- target_id %>%
  left_join(df_nodes, by = c("target_id" = "label"))
  
rm(i, nodes1, nodes2, combinations, list, edges)
  
```


## 2d. Create JSON object containing both 'items' and 'links'

```{r}

# Create list
list <- list()
list$network <- list(items = items,
                     links = links)

# Save as json file
json_object <- toJSON(list, pretty = TRUE, auto_unbox = TRUE)
write(json_object, "../output/network_domains.json")

rm(list, json_object, items, links)

```

